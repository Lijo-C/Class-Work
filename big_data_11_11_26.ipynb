{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUxiP82YRwHPslKhnaReNw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lijo-C/Class-Work/blob/main/big_data_11_11_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHuW7BDNphc6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a11db106",
        "outputId": "14cbd911-7a60-4851-8de7-bf0f8e7bcf35"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17c2be9a",
        "outputId": "4af655b4-1594-44e1-d101-4a42dd8ddb3c"
      },
      "source": [
        "# Define a list of words\n",
        "word_list = ['this', 'is', 'a', 'sample', 'sentence', 'with', 'some', 'common', 'words', 'and', 'also', 'important', 'terms']\n",
        "\n",
        "# Download the stopwords corpus if not already present\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get the English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out the stop words from the word list\n",
        "filtered_words = [word for word in word_list if word.lower() not in stop_words]\n",
        "\n",
        "# Print the original and filtered lists\n",
        "print(\"Original word list:\", word_list)\n",
        "print(\"Filtered word list (without stopwords):\", filtered_words)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original word list: ['this', 'is', 'a', 'sample', 'sentence', 'with', 'some', 'common', 'words', 'and', 'also', 'important', 'terms']\n",
            "Filtered word list (without stopwords): ['sample', 'sentence', 'common', 'words', 'also', 'important', 'terms']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "new_cell_2",
        "outputId": "e09d2fa6-0c7d-44f6-df88-7408c2f1fd84"
      },
      "source": [
        "# Define a sample text\n",
        "sample_text = \"This is an example sentence that demonstrates how to use NLTK for tokenization and stopword removal.\"\n",
        "\n",
        "# Step 1: Tokenize the text into words using whitespace tokenization\n",
        "words = sample_text.split()\n",
        "\n",
        "# Step 2: Get the English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Step 3: Filter out the stop words from the tokenized list\n",
        "filtered_words_from_text = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original text:\", sample_text)\n",
        "print(\"Tokenized words (whitespace):\", words)\n",
        "print(\"Filtered words (without stopwords):\", filtered_words_from_text)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: This is an example sentence that demonstrates how to use NLTK for tokenization and stopword removal.\n",
            "Tokenized words (whitespace):\n",
            "Filtered words (without stopwords):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None,\n",
              " ['example',\n",
              "  'sentence',\n",
              "  'demonstrates',\n",
              "  'use',\n",
              "  'NLTK',\n",
              "  'tokenization',\n",
              "  'stopword',\n",
              "  'removal.'])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa12a9cd",
        "outputId": "f064368f-608f-473a-f39a-537589d06b14"
      },
      "source": [
        "# Install spaCy\n",
        "!pip install spacy\n",
        "\n",
        "# Download the English language model for spaCy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m101.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec89d249",
        "outputId": "2c9305cb-3f34-4c23-ac64-73cb243a3f73"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"SpaCy English model not found. Please run '!python -m spacy download en_core_web_sm' first.\")\n",
        "    raise\n",
        "\n",
        "# Define a sample text\n",
        "sample_text_spacy = \"This is an example sentence that demonstrates how to use spaCy for tokenization and stopword removal.\"\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(sample_text_spacy)\n",
        "\n",
        "# Step 1: Tokenize and filter out stopwords and punctuation\n",
        "filtered_tokens_spacy = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original text:\", sample_text_spacy)\n",
        "print(\"SpaCy Tokens (including stopwords and punctuation):\", [token.text for token in doc])\n",
        "print(\"Filtered tokens (without stopwords and punctuation):\", filtered_tokens_spacy)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: This is an example sentence that demonstrates how to use spaCy for tokenization and stopword removal.\n",
            "SpaCy Tokens (including stopwords and punctuation):\n",
            "Filtered tokens (without stopwords and punctuation):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None,\n",
              " ['example',\n",
              "  'sentence',\n",
              "  'demonstrates',\n",
              "  'use',\n",
              "  'spaCy',\n",
              "  'tokenization',\n",
              "  'stopword',\n",
              "  'removal'])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GysnTerawX_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93520452"
      },
      "source": [
        "# Install the textblob library\n",
        "!pip install textblob\n",
        "\n",
        "# Download necessary NLTK data for textblob (if not already present)\n",
        "# TextBlob relies on NLTK for some of its functionalities\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('corpora/brown')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b2e7d5d"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Define a sample sentence with spelling errors\n",
        "sample_sentence = \"This is an exampel of a sentece with some speling errurs.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(sample_sentence)\n",
        "\n",
        "# Perform spelling correction\n",
        "corrected_sentence = blob.correct()\n",
        "\n",
        "# Print the original and corrected sentences\n",
        "print(\"Original sentence:\", sample_sentence)\n",
        "print(\"Corrected sentence:\", corrected_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aed3557",
        "outputId": "df1ca337-4a19-4e59-8a9d-fd9800d4cc74"
      },
      "source": [
        "# Install the python-Levenshtein library\n",
        "!pip install python-Levenshtein"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.27.3 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3yJuQut-zwyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beadebbb",
        "outputId": "6a352a49-275a-4a7a-ba5d-0306aa4d701f"
      },
      "source": [
        "import nltk\n",
        "# If not already downloaded, uncomment and run:\n",
        "# nltk.download('punkt') # edit_distance might implicitly need this or similar data\n",
        "\n",
        "# Define a misspelled word\n",
        "mispelled_word_nltk = \"sentece\"\n",
        "\n",
        "# Define a list of candidate words\n",
        "candidate_words_nltk = [\"sentence\", \"sequence\", \"sentient\", \"essence\"]\n",
        "\n",
        "print(f\"Misspelled word: {mispelled_word_nltk}\\n\")\n",
        "print(\"Calculating NLTK Edit Distance to candidate words:\")\n",
        "\n",
        "# Calculate Levenshtein distance for each candidate word using nltk.edit_distance\n",
        "distances_nltk = {}\n",
        "for word in candidate_words_nltk:\n",
        "    dist = nltk.edit_distance(mispelled_word_nltk, word)\n",
        "    distances_nltk[word] = dist\n",
        "    print(f\"  Distance between '{mispelled_word_nltk}' and '{word}': {dist}\")\n",
        "\n",
        "# Find the word with the minimum NLTK Edit Distance\n",
        "best_match_nltk = min(distances_nltk, key=distances_nltk.get)\n",
        "min_distance_nltk = distances_nltk[best_match_nltk]\n",
        "\n",
        "print(f\"\\nThe closest candidate word using NLTK Edit Distance is '{best_match_nltk}' with a distance of {min_distance_nltk}.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misspelled word: sentece\n",
            "\n",
            "Calculating NLTK Edit Distance to candidate words:\n",
            "  Distance between 'sentece' and 'sentence': 1\n",
            "  Distance between 'sentece' and 'sequence': 3\n",
            "  Distance between 'sentece' and 'sentient': 3\n",
            "  Distance between 'sentece' and 'essence': 4\n",
            "\n",
            "The closest candidate word using NLTK Edit Distance is 'sentence' with a distance of 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71561017",
        "outputId": "de35c7fd-bf4b-430b-a45b-5a1ae8454c72"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt') # For word tokenization\n",
        "nltk.download('wordnet') # For WordNetLemmatizer"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e263cca3",
        "outputId": "7ad123c9-533d-4864-ca86-0109c1eb8aa4"
      },
      "source": [
        "# Sample words to demonstrate stemming and lemmatization\n",
        "words = ['runs', 'running', 'runner', 'ran', 'easily', 'fairly', 'apples', 'better', 'best', 'cats', 'cactuses']\n",
        "\n",
        "# --- Stemming ---\n",
        "# Initialize Porter Stemmer\n",
        "pstemmer = PorterStemmer()\n",
        "\n",
        "stemmed_words = [pstemmer.stem(word) for word in words]\n",
        "\n",
        "print(\"--- Stemming ---\")\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words: \", stemmed_words)\n",
        "print(\"\\nStemming reduces words to their root form, but the root might not be a valid word. For example, 'runner' becomes 'runn'.\")\n",
        "\n",
        "# --- Lemmatization ---\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize words (default POS is noun)\n",
        "lemmatized_words_noun = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "# Lemmatize words as verbs (to show context sensitivity)\n",
        "lemmatized_words_verb = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(\"\\n--- Lemmatization ---\")\n",
        "print(\"Original words: \", words)\n",
        "print(\"Lemmatized (default noun):\", lemmatized_words_noun)\n",
        "print(\"Lemmatized (as verb):   \", lemmatized_words_verb)\n",
        "print(\"\\nLemmatization reduces words to their base form (lemma), which is always a valid word. It often requires part-of-speech (POS) tagging for better accuracy. For instance, 'runs', 'running', 'runner', 'ran' all reduce to 'run' when treated as verbs, and 'better'/'best' become 'good'.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Stemming ---\n",
            "Original words: ['runs', 'running', 'runner', 'ran', 'easily', 'fairly', 'apples', 'better', 'best', 'cats', 'cactuses']\n",
            "Stemmed words:  ['run', 'run', 'runner', 'ran', 'easili', 'fairli', 'appl', 'better', 'best', 'cat', 'cactus']\n",
            "\n",
            "Stemming reduces words to their root form, but the root might not be a valid word. For example, 'runner' becomes 'runn'.\n",
            "\n",
            "--- Lemmatization ---\n",
            "Original words:  ['runs', 'running', 'runner', 'ran', 'easily', 'fairly', 'apples', 'better', 'best', 'cats', 'cactuses']\n",
            "Lemmatized (default noun): ['run', 'running', 'runner', 'ran', 'easily', 'fairly', 'apple', 'better', 'best', 'cat', 'cactus']\n",
            "Lemmatized (as verb):    ['run', 'run', 'runner', 'run', 'easily', 'fairly', 'apples', 'better', 'best', 'cat', 'cactuses']\n",
            "\n",
            "Lemmatization reduces words to their base form (lemma), which is always a valid word. It often requires part-of-speech (POS) tagging for better accuracy. For instance, 'runs', 'running', 'runner', 'ran' all reduce to 'run' when treated as verbs, and 'better'/'best' become 'good'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NEXT TASK"
      ],
      "metadata": {
        "id": "ztFLF1f0EdGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NEXT"
      ],
      "metadata": {
        "id": "AE1gXi69EbqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQ4KIsVeEfc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54a872d6",
        "outputId": "99e06f5a-cc75-4e5d-9e37-a6d47a4b93a6"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# Download necessary NLTK data for tokenization and POS tagging\n",
        "nltk.download('punkt') # For word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger') # For pos_tag\n",
        "nltk.download('punkt_tab') # Required for some NLTK tokenization internals\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Explicitly download the English tagger model\n",
        "\n",
        "print(\"--- NLTK Part-of-Speech Tagging ---\")\n",
        "# Define a sample sentence\n",
        "nltk_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Tokenize the sentence\n",
        "tokens = word_tokenize(nltk_sentence)\n",
        "\n",
        "# Perform POS tagging\n",
        "pos_tags_nltk = pos_tag(tokens)\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Sentence:\", nltk_sentence)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"POS Tags (NLTK):\", pos_tags_nltk)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- NLTK Part-of-Speech Tagging ---\n",
            "Original Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Tokens: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
            "POS Tags (NLTK): [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "464456a4",
        "outputId": "0ec795e3-7a04-4cc8-dd39-21d733f15c52"
      },
      "source": [
        "import spacy\n",
        "\n",
        "print(\"\\n--- spaCy Part-of-Speech Tagging ---\")\n",
        "\n",
        "# Load the English language model (make sure it's downloaded: !python -m spacy download en_core_web_sm)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"SpaCy English model 'en_core_web_sm' not found. Please run '!python -m spacy download en_core_web_sm' first.\")\n",
        "    raise\n",
        "\n",
        "# Define a sample sentence\n",
        "spacy_sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the sentence with spaCy\n",
        "doc = nlp(spacy_sentence)\n",
        "\n",
        "# Extract tokens and their POS tags\n",
        "pos_tags_spacy = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Print the results\n",
        "print(\"Original Sentence:\", spacy_sentence)\n",
        "print(\"POS Tags (spaCy):\", pos_tags_spacy)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- spaCy Part-of-Speech Tagging ---\n",
            "Original Sentence: The quick brown fox jumps over the lazy dog.\n",
            "POS Tags (spaCy): [('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azQBbQAjFfpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}